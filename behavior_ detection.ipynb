{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32267327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d9d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 632620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>person</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/behaviours-features-merged/Behav...</td>\n",
       "      <td>Looking_Forward</td>\n",
       "      <td>Looking_Forward/ID6/Forward44_id6_Act1_rgb</td>\n",
       "      <td>ID6</td>\n",
       "      <td>Forward44_id6_Act1_rgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/behaviours-features-merged/Behav...</td>\n",
       "      <td>Looking_Forward</td>\n",
       "      <td>Looking_Forward/ID6/Forward44_id6_Act1_rgb</td>\n",
       "      <td>ID6</td>\n",
       "      <td>Forward44_id6_Act1_rgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/behaviours-features-merged/Behav...</td>\n",
       "      <td>Looking_Forward</td>\n",
       "      <td>Looking_Forward/ID6/Forward44_id6_Act1_rgb</td>\n",
       "      <td>ID6</td>\n",
       "      <td>Forward44_id6_Act1_rgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/behaviours-features-merged/Behav...</td>\n",
       "      <td>Looking_Forward</td>\n",
       "      <td>Looking_Forward/ID6/Forward44_id6_Act1_rgb</td>\n",
       "      <td>ID6</td>\n",
       "      <td>Forward44_id6_Act1_rgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/behaviours-features-merged/Behav...</td>\n",
       "      <td>Looking_Forward</td>\n",
       "      <td>Looking_Forward/ID6/Forward44_id6_Act1_rgb</td>\n",
       "      <td>ID6</td>\n",
       "      <td>Forward44_id6_Act1_rgb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path            label  \\\n",
       "0  /kaggle/input/behaviours-features-merged/Behav...  Looking_Forward   \n",
       "1  /kaggle/input/behaviours-features-merged/Behav...  Looking_Forward   \n",
       "2  /kaggle/input/behaviours-features-merged/Behav...  Looking_Forward   \n",
       "3  /kaggle/input/behaviours-features-merged/Behav...  Looking_Forward   \n",
       "4  /kaggle/input/behaviours-features-merged/Behav...  Looking_Forward   \n",
       "\n",
       "                                        group person                sequence  \n",
       "0  Looking_Forward/ID6/Forward44_id6_Act1_rgb    ID6  Forward44_id6_Act1_rgb  \n",
       "1  Looking_Forward/ID6/Forward44_id6_Act1_rgb    ID6  Forward44_id6_Act1_rgb  \n",
       "2  Looking_Forward/ID6/Forward44_id6_Act1_rgb    ID6  Forward44_id6_Act1_rgb  \n",
       "3  Looking_Forward/ID6/Forward44_id6_Act1_rgb    ID6  Forward44_id6_Act1_rgb  \n",
       "4  Looking_Forward/ID6/Forward44_id6_Act1_rgb    ID6  Forward44_id6_Act1_rgb  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"/kaggle/input/behaviours-features-merged/Behaviors_Features_Final\")\n",
    "\n",
    "# Collect all images recursively and derive labels from behavior folder name.\n",
    "records = []\n",
    "for behavior_dir in sorted([p for p in DATA_ROOT.iterdir() if p.is_dir()]):\n",
    "    behavior = behavior_dir.name  # e.g., 'Looking_Forward'\n",
    "    for id_dir in behavior_dir.glob(\"*\"):\n",
    "        if not id_dir.is_dir(): \n",
    "            continue\n",
    "        for seq_dir in id_dir.glob(\"*\"):\n",
    "            if not seq_dir.is_dir():\n",
    "                continue\n",
    "            # Group key: person+sequence folder to avoid near-duplicate leakage\n",
    "            group_key = f\"{behavior}/{id_dir.name}/{seq_dir.name}\"\n",
    "            for img_path in seq_dir.rglob(\"*.png\"):\n",
    "                records.append({\n",
    "                    \"path\": str(img_path),\n",
    "                    \"label\": behavior,\n",
    "                    \"group\": group_key,\n",
    "                    \"person\": id_dir.name,\n",
    "                    \"sequence\": seq_dir.name,\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Remove 'Standing' label from the dataset\n",
    "df = df[df[\"label\"] != \"Standing\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Total images:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146664d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464108 77042 91470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Looking_Forward',\n",
       " 'Raising_Hand',\n",
       " 'Reading',\n",
       " 'Sleeping',\n",
       " 'Standing',\n",
       " 'Turning_Around',\n",
       " 'Writing']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map class names to indices; keep a clean label list for the model head.\n",
    "class_names = sorted(df[\"label\"].unique())\n",
    "class2idx = {c:i for i,c in enumerate(class_names)}\n",
    "df[\"y\"] = df[\"label\"].map(class2idx)\n",
    "\n",
    "# Split train/val/test based on person\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
    "trainval_idx, test_idx = next(gss.split(df, groups=df[\"person\"]))\n",
    "df_trainval, df_test = df.iloc[trainval_idx].reset_index(drop=True), df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=123)\n",
    "tr_idx, va_idx = next(gss2.split(df_trainval, groups=df_trainval[\"person\"]))\n",
    "df_train, df_val = df_trainval.iloc[tr_idx].reset_index(drop=True), df_trainval.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "print(len(df_train), len(df_val), len(df_test))\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _natural_key(value: str):\n",
    "    return [int(tok) if tok.isdigit() else tok.lower() for tok in re.findall(r\"\\d+|\\D+\", str(value))]\n",
    "\n",
    "def to_sequence_df(df_imgs: pd.DataFrame) -> pd.DataFrame:\n",
    "    grouped = (\n",
    "        df_imgs.groupby(\"group\")\n",
    "        .agg(paths=(\"path\", list), label=(\"label\", \"first\"), y=(\"y\", \"first\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped[\"paths\"] = grouped[\"paths\"].apply(lambda items: sorted(items, key=_natural_key))\n",
    "    return grouped\n",
    "\n",
    "df_train_seq = to_sequence_df(df_train)\n",
    "df_val_seq = to_sequence_df(df_val)\n",
    "df_test_seq = to_sequence_df(df_test)\n",
    "print(len(df_train_seq), len(df_val_seq), len(df_test_seq))\n",
    "\n",
    "seq_counts = df_train_seq.groupby(\"label\").size().reindex(class_names, fill_value=0)\n",
    "seq_counts_clipped = seq_counts.replace(0, 1)\n",
    "sequence_class_weights = (1.0 / seq_counts_clipped)\n",
    "sequence_class_weights = sequence_class_weights / sequence_class_weights.sum() * len(sequence_class_weights)\n",
    "sequence_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224  # Base spatial resolution.\n",
    "\n",
    "# image augmentation (train)\n",
    "train_tfms = T.Compose([\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.5, 1.0)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=10),\n",
    "    T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
    "    T.RandomGrayscale(p=0.1),\n",
    "    T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random'),\n",
    "])\n",
    "\n",
    "valid_tfms = T.Compose([\n",
    "    T.Resize(int(IMG_SIZE * 1.14)),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Return a fixed-length clip of frames for sequence-level classification.\"\"\"\n",
    "    def __init__(self, df_seq: pd.DataFrame, transforms, clip_len: int = 8, train: bool = False):\n",
    "        self.paths_list = df_seq[\"paths\"].tolist()\n",
    "        self.labels = df_seq[\"y\"].astype(int).tolist()\n",
    "        self.transforms = transforms\n",
    "        self.clip_len = clip_len\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths_list)\n",
    "\n",
    "    def _sample_indices_train(self, n_frames: int):\n",
    "        # Random continuous clip for training\n",
    "        if n_frames >= self.clip_len:\n",
    "            max_start = n_frames - self.clip_len\n",
    "            start = random.randint(0, max_start)\n",
    "            return list(range(start, start + self.clip_len))\n",
    "        # Pad with the last frame if not enough\n",
    "        return list(range(n_frames)) + [n_frames - 1] * (self.clip_len - n_frames)\n",
    "\n",
    "    def _sample_indices_eval(self, n_frames: int):\n",
    "        # Deterministic uniform sampling for validation/test\n",
    "        if n_frames >= self.clip_len:\n",
    "            return np.linspace(0, n_frames - 1, self.clip_len).astype(int).tolist()\n",
    "        return list(range(n_frames)) + [n_frames - 1] * (self.clip_len - n_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        paths = self.paths_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        n = len(paths)\n",
    "        indices = self._sample_indices_train(n) if self.train else self._sample_indices_eval(n)\n",
    "\n",
    "        frames = []\n",
    "        for frame_idx in indices:\n",
    "            img = Image.open(paths[frame_idx]).convert(\"RGB\")\n",
    "            frames.append(self.transforms(img))\n",
    "        clip = torch.stack(frames, dim=0)\n",
    "        return clip, label\n",
    "\n",
    "CLIP_LEN = 8\n",
    "train_ds = SequenceDataset(df_train_seq, train_tfms, clip_len=CLIP_LEN, train=True)\n",
    "val_ds   = SequenceDataset(df_val_seq,   valid_tfms, clip_len=CLIP_LEN, train=False)\n",
    "test_ds  = SequenceDataset(df_test_seq,  valid_tfms, clip_len=CLIP_LEN, train=False)\n",
    "\n",
    "seq_weight_lookup = sequence_class_weights.to_dict()\n",
    "train_sample_weights = df_train_seq[\"label\"].map(seq_weight_lookup).astype(float).values\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    torch.as_tensor(train_sample_weights, dtype=torch.double),\n",
    "    num_samples=len(train_sample_weights),\n",
    "    replacement=True,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0  # Avoid notebook exceptions by setting to 0\n",
    "PIN_MEMORY = False\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2160fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name convnext_small_in22ft1k to current convnext_small.fb_in22k_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3208377d477e42508e81055fd905412b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/201M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80/1084503723.py:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f31ca13606b43038cea312da24c4837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80/1084503723.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_80/1084503723.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mbest_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mva_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_80/1084503723.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(dataloader, train)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N_CLASSES = len(class_names)\n",
    "MODEL_NAME = \"convnext_small.fb_in22k_ft_in1k\"\n",
    "\n",
    "class TemporalMeanNet(nn.Module):\n",
    "    def __init__(self, backbone_name: str, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool=\"avg\",\n",
    "            drop_path_rate=0.2,\n",
    "        )\n",
    "        self.embed_dim = self.backbone.num_features\n",
    "        # Add regularization to the head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(self.embed_dim),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(self.embed_dim, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, clips):\n",
    "        # clips: (batch, time, channels, height, width)\n",
    "        b, t, c, h, w = clips.shape\n",
    "        clips = clips.view(b * t, c, h, w)\n",
    "        feats = self.backbone(clips)  # (b * t, feat_dim)\n",
    "        feats = feats.view(b, t, -1).mean(dim=1)\n",
    "        return self.head(feats)\n",
    "\n",
    "model = TemporalMeanNet(MODEL_NAME, N_CLASSES).to(device)\n",
    "use_amp = device.type == \"cuda\"\n",
    "\n",
    "# Mixup/CutMix + SoftTargetCrossEntropy\n",
    "from timm.data import Mixup\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "\n",
    "mixup_fn = Mixup(\n",
    "    mixup_alpha=0.2, cutmix_alpha=1.0, prob=0.5, switch_prob=0.0,\n",
    "    mode='batch', label_smoothing=0.0, num_classes=N_CLASSES\n",
    ")\n",
    "criterion = SoftTargetCrossEntropy()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4, weight_decay=5e-2)\n",
    "EPOCHS = 10\n",
    "steps_per_epoch = len(train_dl)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=2e-4, epochs=EPOCHS, steps_per_epoch=steps_per_epoch)\n",
    "metric_acc = MulticlassAccuracy(num_classes=N_CLASSES).to(device)\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "\n",
    "def run_one_epoch(dataloader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    metric_acc.reset()\n",
    "    pbar = tqdm(dataloader, leave=False)\n",
    "    for clips, targets in pbar:\n",
    "        clips = clips.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        if train and mixup_fn is not None:\n",
    "            hard_targets = targets.clone()\n",
    "        with torch.set_grad_enabled(train):\n",
    "            with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                if train and mixup_fn is not None:\n",
    "                    clips, soft_targets = mixup_fn(clips, targets)\n",
    "                    logits = model(clips)\n",
    "                    loss = criterion(logits, soft_targets)\n",
    "                else:\n",
    "                    logits = model(clips)\n",
    "                    loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            pbar.set_postfix(loss=loss.item(), lr=f\"{current_lr:.2e}\")\n",
    "        else:\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        total_loss += loss.item() * clips.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        # Calculate accuracy with hard targets during Mixup\n",
    "        metric_acc.update(preds, hard_targets if (train and mixup_fn is not None) else targets)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_acc = metric_acc.compute().item()\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "best_val = 0.0\n",
    "patience, bad_epochs = 3, 0  # Early stopping\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc = run_one_epoch(train_dl, train=True)\n",
    "    va_loss, va_acc = run_one_epoch(val_dl, train=False)\n",
    "    print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        bad_epochs = 0\n",
    "        torch.save({\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"class_names\": class_names,\n",
    "            \"clip_len\": CLIP_LEN,\n",
    "        }, \"/kaggle/working/best_model_convnext_small_in22ft1k_sequence_based_group_split_augmented.pth\")\n",
    "        print(\"Saved new best model.\")\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        if bad_epochs >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5bc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b573010aa1549bb9ae0977c0fab8b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/566 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Looking_Forward       0.99      1.00      1.00      4333\n",
      "   Raising_Hand       1.00      0.99      0.99      2979\n",
      "        Reading       1.00      1.00      1.00      6975\n",
      "       Sleeping       1.00      1.00      1.00      8401\n",
      "       Standing       1.00      1.00      1.00      1195\n",
      " Turning_Around       1.00      1.00      1.00      5939\n",
      "       Writting       1.00      1.00      1.00      6361\n",
      "\n",
      "       accuracy                           1.00     36183\n",
      "      macro avg       1.00      1.00      1.00     36183\n",
      "   weighted avg       1.00      1.00      1.00     36183\n",
      "\n",
      "[[4333    0    0    0    0    0    0]\n",
      " [  22 2952    0    5    0    0    0]\n",
      " [   0    2 6966    0    0    6    1]\n",
      " [   0    2    0 8377    0   21    1]\n",
      " [   0    1    0    4 1190    0    0]\n",
      " [   0    0    0    0    0 5939    0]\n",
      " [   0    0    0    0    0    0 6361]]\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"/kaggle/working/best_model_convnext_small_in22ft1k_sequence_based_group_split_augmented.pth\", map_location=device)\n",
    "eval_model = TemporalMeanNet(ckpt[\"model_name\"], len(ckpt[\"class_names\"])).to(device)\n",
    "eval_model.load_state_dict(ckpt[\"state_dict\"])\n",
    "eval_model.eval()\n",
    "all_preds, all_targs = [], []\n",
    "with torch.no_grad():\n",
    "    for clips, targets in tqdm(test_dl):\n",
    "        clips = clips.to(device)\n",
    "        logits = eval_model(clips)\n",
    "        preds = logits.argmax(1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_targs.append(targets.numpy())\n",
    "\n",
    "y_pred = np.concatenate(all_preds)\n",
    "y_true = np.concatenate(all_targs)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
