{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7217892,"sourceType":"datasetVersion","datasetId":4177367}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"32267327","cell_type":"code","source":"import os\nimport glob\nimport random\nimport math\nimport json\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.model_selection import GroupShuffleSplit\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torchvision.transforms as T\nimport timm\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torchmetrics.classification import MulticlassAccuracy\nfrom tqdm.auto import tqdm\nimport torch.nn.functional as F\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\n","metadata":{},"outputs":[],"execution_count":3},{"id":"7c7d9d5c","cell_type":"code","source":"DATA_ROOT = Path(\"/kaggle/input/classroom-student-behaviors/Behaviors_Features\")\n\n# Collect all images recursively and derive labels from behavior folder name.\nrecords = []\nfor behavior_dir in sorted([p for p in DATA_ROOT.iterdir() if p.is_dir()]):\n    behavior = behavior_dir.name  # e.g., 'Looking_Forward'\n    for id_dir in behavior_dir.glob(\"*\"):\n        if not id_dir.is_dir(): \n            continue\n        for seq_dir in id_dir.glob(\"*\"):\n            if not seq_dir.is_dir():\n                continue\n            # Group key: person+sequence folder to avoid near-duplicate leakage\n            group_key = f\"{behavior}/{id_dir.name}/{seq_dir.name}\"\n            for img_path in seq_dir.rglob(\"*.png\"):\n                records.append({\n                    \"path\": str(img_path),\n                    \"label\": behavior,\n                    \"group\": group_key,\n                    \"person\": id_dir.name,\n                    \"sequence\": seq_dir.name,\n                })\n\ndf = pd.DataFrame(records)\nprint(\"Total images:\", len(df))\ndf.head()\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total images: 252223\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>label</th>\n","      <th>group</th>\n","      <th>person</th>\n","      <th>sequence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/kaggle/input/classroom-student-behaviors/Beha...</td>\n","      <td>Looking_Forward</td>\n","      <td>Looking_Forward/ID4/Forward28_id4_Act1_rgb</td>\n","      <td>ID4</td>\n","      <td>Forward28_id4_Act1_rgb</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/kaggle/input/classroom-student-behaviors/Beha...</td>\n","      <td>Looking_Forward</td>\n","      <td>Looking_Forward/ID4/Forward28_id4_Act1_rgb</td>\n","      <td>ID4</td>\n","      <td>Forward28_id4_Act1_rgb</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/kaggle/input/classroom-student-behaviors/Beha...</td>\n","      <td>Looking_Forward</td>\n","      <td>Looking_Forward/ID4/Forward28_id4_Act1_rgb</td>\n","      <td>ID4</td>\n","      <td>Forward28_id4_Act1_rgb</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/kaggle/input/classroom-student-behaviors/Beha...</td>\n","      <td>Looking_Forward</td>\n","      <td>Looking_Forward/ID4/Forward28_id4_Act1_rgb</td>\n","      <td>ID4</td>\n","      <td>Forward28_id4_Act1_rgb</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/kaggle/input/classroom-student-behaviors/Beha...</td>\n","      <td>Looking_Forward</td>\n","      <td>Looking_Forward/ID4/Forward28_id4_Act1_rgb</td>\n","      <td>ID4</td>\n","      <td>Forward28_id4_Act1_rgb</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                path            label  \\\n","0  /kaggle/input/classroom-student-behaviors/Beha...  Looking_Forward   \n","1  /kaggle/input/classroom-student-behaviors/Beha...  Looking_Forward   \n","2  /kaggle/input/classroom-student-behaviors/Beha...  Looking_Forward   \n","3  /kaggle/input/classroom-student-behaviors/Beha...  Looking_Forward   \n","4  /kaggle/input/classroom-student-behaviors/Beha...  Looking_Forward   \n","\n","                                        group person                sequence  \n","0  Looking_Forward/ID4/Forward28_id4_Act1_rgb    ID4  Forward28_id4_Act1_rgb  \n","1  Looking_Forward/ID4/Forward28_id4_Act1_rgb    ID4  Forward28_id4_Act1_rgb  \n","2  Looking_Forward/ID4/Forward28_id4_Act1_rgb    ID4  Forward28_id4_Act1_rgb  \n","3  Looking_Forward/ID4/Forward28_id4_Act1_rgb    ID4  Forward28_id4_Act1_rgb  \n","4  Looking_Forward/ID4/Forward28_id4_Act1_rgb    ID4  Forward28_id4_Act1_rgb  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"execution_count":4},{"id":"a146664d","cell_type":"code","source":"# Map class names to indices; keep a clean label list for the model head.\nclass_names = sorted(df[\"label\"].unique())\nclass2idx = {c:i for i,c in enumerate(class_names)}\ndf[\"y\"] = df[\"label\"].map(class2idx)\n\n# First split: train+val vs test by groups (sequence level).\ngss = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\ntrainval_idx, test_idx = next(gss.split(df, groups=df[\"group\"]))\ndf_trainval, df_test = df.iloc[trainval_idx].reset_index(drop=True), df.iloc[test_idx].reset_index(drop=True)\n\n# Second split: train vs val (still grouped to prevent leakage).\ngss2 = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=123)\ntr_idx, va_idx = next(gss2.split(df_trainval, groups=df_trainval[\"group\"]))\ndf_train, df_val = df_trainval.iloc[tr_idx].reset_index(drop=True), df_trainval.iloc[va_idx].reset_index(drop=True)\n\nprint(len(df_train), len(df_val), len(df_test))\nclass_names\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["184571 31469 36183\n"]},{"data":{"text/plain":["['Looking_Forward',\n"," 'Raising_Hand',\n"," 'Reading',\n"," 'Sleeping',\n"," 'Standing',\n"," 'Turning_Around',\n"," 'Writting']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"execution_count":5},{"id":"752b0d0e","cell_type":"code","source":"IMG_SIZE = 224  # You can try 256 or 384 later.\n\n# Training-time augmentations for robustness.\ntrain_tfms = T.Compose([\n    T.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomRotation(degrees=10),\n    T.ToTensor(),\n    T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),  # ImageNet stats\n])\n\n# Validation/Test transforms must be deterministic.\nvalid_tfms = T.Compose([\n    T.Resize(int(IMG_SIZE*1.14)),\n    T.CenterCrop(IMG_SIZE),\n    T.ToTensor(),\n    T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n])\n\nclass BehaviorDataset(Dataset):\n    # This dataset reads image paths and returns (tensor, label)\n    def __init__(self, df, transforms):\n        self.paths = df[\"path\"].tolist()\n        self.labels = df[\"y\"].astype(int).tolist()\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        y = self.labels[idx]\n        img = Image.open(p).convert(\"RGB\")\n        img = self.transforms(img)\n        return img, y\n\ntrain_ds = BehaviorDataset(df_train, train_tfms)\nval_ds   = BehaviorDataset(df_val, valid_tfms)\ntest_ds  = BehaviorDataset(df_test, valid_tfms)\n\nBATCH_SIZE = 64\nNUM_WORKERS = 2  # Kaggle often limits >2; adjust if needed.\n\ntrain_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\nval_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\ntest_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n","metadata":{},"outputs":[],"execution_count":6},{"id":"af2160fa","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nN_CLASSES = len(class_names)\nMODEL_NAME = \"efficientnet_b0\"  # Try 'convnext_tiny', 'vit_base_patch16_224' later.\n\n# Create a timm model with a classification head sized to our classes.\nmodel = timm.create_model(MODEL_NAME, pretrained=True, num_classes=N_CLASSES)\nmodel.to(device)\n\n# Loss, optimizer, scheduler, metrics.\ncriterion = nn.CrossEntropyLoss()\noptimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\nEPOCHS = 1 # Better with 10\nsteps_per_epoch = math.ceil(len(train_dl.dataset)/BATCH_SIZE)\nscheduler = OneCycleLR(optimizer, max_lr=1e-3, epochs=EPOCHS, steps_per_epoch=steps_per_epoch)\nmetric_acc = MulticlassAccuracy(num_classes=N_CLASSES).to(device)\n\ndef run_one_epoch(dataloader, train=True):\n    # This function runs one epoch for either training or validation.\n    model.train(train)\n    total_loss = 0.0\n    metric_acc.reset()\n    pbar = tqdm(dataloader, leave=False)\n    for x, y in pbar:\n        x = x.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n\n        with torch.set_grad_enabled(train):\n            logits = model(x)\n            loss = criterion(logits, y)\n\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n            optimizer.step()\n            scheduler.step()\n\n        total_loss += loss.item() * x.size(0)\n        preds = logits.argmax(dim=1)\n        metric_acc.update(preds, y)\n        pbar.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / len(dataloader.dataset)\n    avg_acc = metric_acc.compute().item()\n    return avg_loss, avg_acc\n\nbest_val = 0.0\nfor epoch in range(1, EPOCHS+1):\n    tr_loss, tr_acc = run_one_epoch(train_dl, train=True)\n    va_loss, va_acc = run_one_epoch(val_dl,   train=False)\n    print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n    if va_acc > best_val:\n        best_val = va_acc\n        torch.save({\n            \"model_name\": MODEL_NAME,\n            \"state_dict\": model.state_dict(),\n            \"class_names\": class_names\n        }, \"/kaggle/working/best_model.pth\")\n        print(\"Saved new best model.\")\n","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b4ae282645b42669b4df1f6a4add98e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df6c66f4fe404affa972074ec9c8c868","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2884 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c9f189d216f44df9deadea2824f9541","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/492 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 01 | train loss 0.0469 acc 0.9823 | val loss 0.0077 acc 0.9963\n","Saved new best model.\n"]}],"execution_count":7},{"id":"64fb1f43","cell_type":"code","source":"# Code from here is just a playground, skip them all\n\n# Path to your saved version’s files\nMODEL_DIR = \"/kaggle/input/classroom-behavior-model\"  # ← change to your actual notebook input path\n\n# Load class names\nwith open(f\"{MODEL_DIR}/label_map.json\", \"r\") as f:\n    class_names = json.load(f)\n\n# Recreate the model (same architecture used before)\nMODEL_NAME = \"efficientnet_b0\"\nN_CLASSES = len(class_names)\nmodel = timm.create_model(MODEL_NAME, pretrained=False, num_classes=N_CLASSES)\n\n# Load saved weights\nckpt = torch.load(f\"{MODEL_DIR}/best_model.pth\", map_location=\"cuda\")\nmodel.load_state_dict(ckpt[\"state_dict\"])\nmodel.eval().to(\"cuda\")\n\nprint(\"✅ Model reloaded and ready for inference!\")\n","metadata":{},"outputs":[],"execution_count":null},{"id":"02b5bc14","cell_type":"code","source":"# Load best weights just in case.\nckpt = torch.load(\"/kaggle/working/best_model.pth\", map_location=device)\nmodel.load_state_dict(ckpt[\"state_dict\"])\nmodel.eval()\n\nall_preds, all_targs = [], []\nwith torch.no_grad():\n    for x, y in tqdm(test_dl):\n        x = x.to(device)\n        logits = model(x)\n        preds = logits.argmax(1).cpu().numpy()\n        all_preds.append(preds)\n        all_targs.append(y.numpy())\n\ny_pred = np.concatenate(all_preds)\ny_true = np.concatenate(all_targs)\n\nprint(classification_report(y_true, y_pred, target_names=class_names))\nprint(confusion_matrix(y_true, y_pred))\n","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b573010aa1549bb9ae0977c0fab8b9a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/566 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["                 precision    recall  f1-score   support\n","\n","Looking_Forward       0.99      1.00      1.00      4333\n","   Raising_Hand       1.00      0.99      0.99      2979\n","        Reading       1.00      1.00      1.00      6975\n","       Sleeping       1.00      1.00      1.00      8401\n","       Standing       1.00      1.00      1.00      1195\n"," Turning_Around       1.00      1.00      1.00      5939\n","       Writting       1.00      1.00      1.00      6361\n","\n","       accuracy                           1.00     36183\n","      macro avg       1.00      1.00      1.00     36183\n","   weighted avg       1.00      1.00      1.00     36183\n","\n","[[4333    0    0    0    0    0    0]\n"," [  22 2952    0    5    0    0    0]\n"," [   0    2 6966    0    0    6    1]\n"," [   0    2    0 8377    0   21    1]\n"," [   0    1    0    4 1190    0    0]\n"," [   0    0    0    0    0 5939    0]\n"," [   0    0    0    0    0    0 6361]]\n"]}],"execution_count":8},{"id":"2ae3e68f","cell_type":"code","source":"\n# Reuse valid_tfms for deterministic preprocessing.\ndef predict_image(path):\n    # This function predicts the behavior class for a single image path.\n    img = Image.open(path).convert(\"RGB\")\n    x = valid_tfms(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        logits = model(x)\n        probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n    top = probs.argmax()\n    return class_names[top], float(probs[top]), {c: float(p) for c,p in zip(class_names, probs)}\n\n# Example:\n# predict_image(df_test.iloc[0][\"path\"])\n","metadata":{},"outputs":[],"execution_count":9},{"id":"2f928e6a","cell_type":"code","source":"# Save class names for future inference.\nwith open(\"/kaggle/working/label_map.json\", \"w\") as f:\n    json.dump(class_names, f, indent=2)\n\n# The model weights are already at /kaggle/working/best_model.pth\n","metadata":{},"outputs":[],"execution_count":10},{"id":"a891b45d","cell_type":"code","source":"# Check how many 'group' values overlap between train and test\nset_train_groups = set(df_train[\"group\"])\nset_test_groups = set(df_test[\"group\"])\n\nlen(set_train_groups & set_test_groups)\n","metadata":{},"outputs":[{"data":{"text/plain":["0"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"execution_count":11}]}